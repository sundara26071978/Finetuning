# -*- coding: utf-8 -*-
"""Bert_Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZuhuyUkg9TfwCQ_dq-FyFxNxRZvKWdrX
"""

!pip install -q transformers[torch] datasets accelerate tokenizers seqeval evaluate

import datasets
import numpy as np
from transformers import BertTokenizerFast
from transformers import DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification

import os
os.environ["WANDB_DISABLED"] = "true"

"""CoNLL-2003 Dataset Kya Hai?

CoNLL-2003 ek NER (Named Entity Recognition) dataset hai, jo 2003 ke CoNLL (Conference on Computational Natural Language Learning) me introduce kiya gaya tha. Yeh dataset news articles ka collection hai aur 4 entity types detect karne ke liye use hota hai:

PER → Person (e.g., "Elon Musk")

LOC → Location (e.g., "India", "New York")

ORG → Organization (e.g., "Google", "NASA")

MISC → Miscellaneous (e.g., "Olympics", "iPhone")

"""

conll2003 = datasets.load_dataset("conll2003")

conll2003

conll2003["train"]

conll2003["train"].description

conll2003["train"]. features

conll2003["train"][0]

conll2003["train"]. features['ner_tags']

conll2003["train"]. features['chunk_tags']

conll2003["train"]. features['pos_tags']

"""The (DT)        → Determiner

quick (JJ)      → Adjective

brown (JJ)      → Adjective

fox (NN)        → Noun (Singular)

jumps (VBZ)     → Verb (3rd person singular)

over (IN)       → Preposition

the (DT)        → Determiner

lazy (JJ)       → Adjective

dog (NN)        → Noun (Singular)

"""

example_text=conll2003['train'][0]

example_text

example_text["tokens"]

model = "bert-base-uncased"
tokenizer = BertTokenizerFast.from_pretrained(model)

tokenized_id = tokenizer(example_text["tokens"],is_split_into_words=True)

tokenized_id

tokenized_id["input_ids"]

tokens = tokenizer.convert_ids_to_tokens(tokenized_id["input_ids"])

tokens

example_text['ner_tags']

def tokenize_and_align_labels(examples, label_all_tokens=True):

    #tokeinze ids
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []


    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        # word_ids() => Return a list mapping the tokens
        # to their actual word in the initial sentence.
        # It Returns a list indicating the word corresponding to each token.

        previous_word_idx = None
        label_ids = []
        # Special tokens like `` and `<\s>` are originally mapped to None
        # We need to set the label to -100 so they are automatically ignored in the loss function.
        for word_idx in word_ids:
            if word_idx is None:
                # set –100 as the label for these special tokens
                label_ids.append(-100)

            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            elif word_idx != previous_word_idx:
                # if current word_idx is != prev then its the most regular case
                # and add the corresponding token
                label_ids.append(label[word_idx])
            else:
                # to take care of sub-words which have the same word_idx
                # set -100 as well for them, but only if label_all_tokens == False
                label_ids.append(label[word_idx] if label_all_tokens else -100)
                # mask the subword representations after the first subword

            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

conll2003["train"][0]

q=tokenize_and_align_labels(conll2003["train"][0:1])

for token, label in zip(tokenizer.convert_ids_to_tokens(q["input_ids"][0]),q["labels"][0]):
    print(f"{token:_<40} {label}")

tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)

tokenized_datasets["train"][4]

model = AutoModelForTokenClassification.from_pretrained("bert-base-uncased",num_labels=9)

label_list=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']

import evaluate
metric = evaluate.load("seqeval")

example=conll2003["train"][0]

conll2003["train"].features

conll2003["train"].features["ner_tags"].feature. names

example

label_list=conll2003["train"].features["ner_tags"].feature. names

label_list

for i in example["ner_tags"]:
  print(i)

['EU','rejects','German','call','to','boycott','British','lamb','.']

['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']

labels = [label_list[i] for i in example["ner_tags"]]
labels

metric.compute(predictions=[labels],references=[labels])

results=metric.compute(predictions=[labels],references=[labels])

print(list(results.keys()))

print(results["overall_f1"])

from transformers import TrainingArguments, Trainer

args=TrainingArguments(
    "test-ner",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    weight_decay=0.01,
    report_to="none"  # Disable wandb logging
)

data_collator=DataCollatorForTokenClassification(tokenizer)

def compute_metrics(eval_preds):
    pred_logits, labels = eval_preds

    pred_logits = np.argmax(pred_logits, axis=2)
    # the logits and the probabilities are in the same order,
    # so we don’t need to apply the softmax

    # We remove all the values where the label is -100
    predictions = [
        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(pred_logits, labels)
    ]

    true_labels = [
      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]
       for prediction, label in zip(pred_logits, labels)
   ]
    results = metric.compute(predictions=predictions, references=true_labels)

    return {
          "precision": results["overall_precision"],
          "recall": results["overall_recall"],
          "f1": results["overall_f1"],
          "accuracy": results["overall_accuracy"],
  }

trainer=Trainer(
   model,
   args,
   train_dataset=tokenized_datasets["train"],
   eval_dataset=tokenized_datasets["validation"],
   data_collator=data_collator,
   tokenizer=tokenizer,
   compute_metrics=compute_metrics
)

trainer.train()

model.save_pretrained("ner_model")

import shutil

# Model folder ka naam jo tu save kar chuka hai
model_folder = "/content/ner_model"

# Model ko zip me convert karo
shutil.make_archive("sunj_bert_model", 'zip', model_folder)

print("Model successfully zipped as bert_model.zip")

from google.colab import files

# ZIP file ko download karne ke liye
files.download("sunj_bert_model.zip")

import zipfile

# ZIP file ka path
zip_path = "sunj_bert_model.zip"

# Extract karne ke liye
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("./extracted_model")

print("Model successfully extracted!")

tokenizer.save_pretrained("tokenizer")

import json

config=json.load(open("/content/extracted_model/config.json"))

config

conll2003["train"].features["ner_tags"].feature. names

id2label = {str(i): label for i,label in enumerate(label_list)}

label2id = {label: str(i) for i,label in enumerate(label_list)}

config["id2label"]=id2label

config["label2id"] = label2id

json.dump(config,open("/content/extracted_model/config.json","w"))

config=json.load(open("/content/extracted_model/config.json"))
config

"""# Tansformer Pipeline"""

from transformers import pipeline

model = "bert-base-uncased"

tokenizer = BertTokenizerFast.from_pretrained(model)

model_fine_tuned=AutoModelForTokenClassification.from_pretrained("/content/extracted_model")

nlp_pipeline=pipeline("ner",model=model_fine_tuned,tokenizer=tokenizer)

example="Sunny is Data Scientist and Generative AI Engineer"

nlp_pipeline(example)

example2="apple launch mobile while eating apple which taste like orange"

nlp_pipeline(example2)

example="my name is suuny savita"

nlp_pipeline(example)

example2="apple launch mobile while"

nlp_pipeline(example2)

example="apple founder loves eating apple"

nlp_pipeline(example)

example="Microsoft Windows created their software by idea that came from the window of the house"

nlp_pipeline(example)

example= "sunny is a founder of facebook and microsoft"

nlp_pipeline(example)

1. pretrain model
2. supervise fiunetuing(NER data)
3. ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']

! pip install huggingface_hub

from huggingface_hub import notebook_login

notebook_login()

from transformers import AutoModelForTokenClassification, AutoTokenizer
from huggingface_hub import HfApi

# Model directory (jisme fine-tuned model hai)
model_dir = "/content/extracted_model"

# Push model to Hugging Face
model_fine_tuned.push_to_hub("sunjupskilling/sunj-ner-model")

# tokenizer.push_to_hub("your_username/your_model_name")

# print("✅ Model pushed successfully!")