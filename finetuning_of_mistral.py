# -*- coding: utf-8 -*-
"""FInetuning_of_Mistral.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EUZqZyMPpZuUpHB1YTWwRhsaRqpmedF8
"""

!pip install accelerate peft bitsandbytes trl py7zr auto-gptq optimum transformers

# from huggingface_hub import login

# login(token="your_huggingface_token")

from huggingface_hub import notebook_login
notebook_login()

import torch
import os
from datasets import load_dataset,Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments,GPTQConfig
from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer

data=load_dataset("samsum",split="train")

data

data.to_pandas()

data.to_pandas().iloc[:,1][0]

data.to_pandas().iloc[:,2][0]

data_df=data.to_pandas()

"""LLM Input:

###Human: Summarize this following dialogue:  

Amanda: I baked cookies. Do you want some?  
Jerry: Yes, Iâ€™d love some.  

### AI Assistant(Model):

Amanda baked cookies and will bring Jerry some.

"""

data_df.head()

data_df["text"]=data_df[["dialogue","summary"]].apply(lambda x:"###Human: Summarize this following dialogue: " + x["dialogue"] + "\n###Assistant: " +x["summary"] ,axis=1)

data_df.head()

print(data_df.iloc[0][3])

data_df

data=Dataset.from_pandas(data_df)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

tokenizer.pad_token_id

tokenizer.pad_token

tokenizer.eos_token_id

tokenizer.eos_token

tokenizer.pad_token=tokenizer.eos_token

#tomorrow i will explain you this thing along with mathematics concepts
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,    # Enable 4-bit quantization
    bnb_4bit_compute_dtype=torch.float16,  # Use fp16 for computation
    bnb_4bit_use_double_quant=True,  # Use double quantization for memory efficiency
)

model=AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("TheBloke/Mistral-7B-Instruct-v0.1-GPTQ")
quantization_config_loading = GPTQConfig(bits=4, tokenizer=tokenizer)
model = AutoModelForCausalLM.from_pretrained(
                          "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ",
                          quantization_config=quantization_config_loading,
                          device_map="auto")

print(model)

model=prepare_model_for_kbit_training(model)

"""### This is my quantize model

#### load the model into the lower precision

## for the memory efficent training
"""

print(model)

"""### LORA"""

## Inside params are all the hyperparameters
peft_config=LoraConfig(
    r=16, lora_alpha=16, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", target_modules=["q_proj", "v_proj"]
)

model=get_peft_model(model,peft_config)

print(model)

traning_arguments = TrainingArguments(
    output_dir="mistral-finetuned-samsum",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    save_strategy="epoch",
    logging_steps=100,
    num_train_epochs=1,
    fp16=True,
    push_to_hub=True,
    report_to="none"
)

data

trainer=SFTTrainer(
    model=model,
    train_dataset=data,
    peft_config=peft_config,
    tokenizer=tokenizer,
    args=traning_arguments,
)

trainer.train()

!cp -r /content/mistral-finetuned-samsum /content/drive/MyDrive/

tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/mistral-finetuned-samsum")

model=AutoPeftModelForCausalLM.from_pretrained("/content/drive/MyDrive/mistral-finetuned-samsum",
                                               low_cpu_mem_usage=True,
                                              return_dict=True,
                                              torch_dtype=torch.float16,
                                              device_map="auto")

"""###Human: Summarize this following dialogue: Sunny: I'm at the railway station in Chennai Karthik: No problems so far? Sunny: no, everything's going smoothly Karthik: good. lets meet there soon!"""

inputs = tokenizer("""
###Human: Summarize this following dialogue: Sunny: I'm at the railway station in Chennai Karthik: No problems so far? Sunny: no, everything's going smoothly Karthik: good. lets meet there soon!
###Assistant: """, return_tensors="pt").to("cuda")

output=model.generate(**inputs,max_new_tokens=100,do_sample=True,top_p=0.9,temperature=0.8)

tokenizer.decode(output[0],skip_special_tokens=True)

"""Human: Summarize this following dialogue: Sunny: I'm at the railway station in Chennai Karthik: No problems so far? Sunny: no, everything's going smoothly Karthik: good. lets meet there soon!

Sunny is at the railway station in Chennai. Karthik will meet him
"""