# -*- coding: utf-8 -*-
"""finetuning_mistral.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXwmmKKT_VybLoLZBjneeJ5MyWDpZM-6

*   Unsupervised pretraining
*   Supervised finetuning
* RLHF (Direct Preference Optimization and Proximal Preference Optimization)

We will Finetune LLM today Mistral, LLAMA, Deepseek.
WE have billions of parameter in these model and we cannot perform supervised fine tuning.
WE will perform PEFT.Parameter efficient finetuning. It takes the subset of trainable parameters from the pretrained model. The attention layer in the transformers have trainable parameters.  which will be used to finetune the weights. LORA will be used.
Adaptors are used to freeze the model weights and allow trainable parameters to be tranined

Quantization + LORA. we reduce the precision of the weights. from FP16 to FP8 or FP4.

Different quantization techniques

- GGML
- GGUF
- GPTQ
"""

! pip install accelerate peft bitsandbytes trl py7zr auto-gptq optimum transformers

from huggingface_hub import notebook_login
notebook_login()

import torch
import os
from datasets import load_dataset,Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments,GPTQConfig
from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer

data= load_dataset("samsum", split="train")

data_df=data.to_pandas()

data_df.head()

data_df["text"]=data_df[["dialogue","summary"]].apply(lambda x:"###Human: Summarize this following dialogue: " + x["dialogue"] + "\n###Assistant: " +x["summary"] ,axis=1)

data_df[["text"]]

data= Dataset.from_pandas(data_df)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

tokenizer.pad_token=tokenizer.eos_token

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,    # Enable 4-bit quantization
    bnb_4bit_compute_dtype=torch.float16,  # Use fp16 for computation
    bnb_4bit_use_double_quant=True,  # Use double quantization for memory efficiency
)

model=AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    quantization_config=quantization_config,
    device_map="auto"
                                     )

model=prepare_model_for_kbit_training(model)

"""LORA"""

peft_config=LoraConfig(
    r=16, lora_alpha=16, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", target_modules=["q_proj", "v_proj"]
)

model=get_peft_model(model,peft_config=peft_config)

traning_arguments = TrainingArguments(
    output_dir="mistral-finetuned-samsum",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    save_strategy="epoch",
    logging_steps=100,
    num_train_epochs=1,
    fp16=True,
    push_to_hub=True,
    report_to="none"
)

trainer=SFTTrainer(
    model=model,
    train_dataset=data,
    peft_config=peft_config,
    tokenizer=tokenizer,
    args=traning_arguments,
)

trainer.train()







